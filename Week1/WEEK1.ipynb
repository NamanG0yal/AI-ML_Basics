{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSduJFktmLPW",
        "outputId": "c85b1885-4963-4037-cdaa-585587624e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8672159212497325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6201583565161567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6048576931307511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:36:51] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"Objective\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8340466509736786\n",
            "      0     1\n",
            "0  2834  1856\n",
            "1  1694  2962\n",
            "      0     1\n",
            "0  4116   574\n",
            "1   667  3989\n",
            "      0     1\n",
            "0  3011  1679\n",
            "1  2014  2642\n",
            "      0     1\n",
            "0  4080   610\n",
            "1   941  3715\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.svm import SVC\n",
        "#Normalisation\n",
        "b=Normalizer()\n",
        "data=pd.read_excel('/content/default of credit card clients.xls')\n",
        "#Imputation in place of feature engineering and data preprocessing\n",
        "imputer= SimpleImputer(missing_values=np.nan,strategy='mean')\n",
        "A=data.iloc[1:,:-1]\n",
        "B=pd.DataFrame(map(int,data.iloc[1:,-1]))\n",
        "A=imputer.fit_transform(A)\n",
        "A=b.transform(A)\n",
        "X_r,Y_r = (A, B)\n",
        "#SMOTE for class imbalance\n",
        "X_resampled, Y_resampled = SMOTE().fit_resample(X_r, Y_r)\n",
        "#Data splitting into test and train\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X_resampled,Y_resampled,test_size=0.2)\n",
        "#Random Forest\n",
        "k=RandomForestClassifier()\n",
        "k.fit(X_train,np.array(Y_train))\n",
        "y_preprf=k.predict(X_test)\n",
        "accuracyrf=accuracy_score(Y_test,y_preprf)\n",
        "print(accuracyrf)\n",
        "#Logistic Regression\n",
        "g=LogisticRegression()\n",
        "g.fit(X_train,np.array(Y_train))\n",
        "y_preplr=g.predict(X_test)\n",
        "accuracylr=accuracy_score(Y_test,y_preplr)\n",
        "print(accuracylr)\n",
        "#Support Vector Machine\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, np.array(Y_train))\n",
        "y_prepsv=model.predict(X_test)\n",
        "accuracysv=accuracy_score(Y_test,y_prepsv)\n",
        "print(accuracysv)\n",
        "#XGBoost\n",
        "dtrain_reg = xgb.DMatrix(X_train, Y_train, enable_categorical=True)\n",
        "dtest_reg = xgb.DMatrix(X_test, Y_test, enable_categorical=True)\n",
        "params={\"Objective\":\"binary:logistic\"} #\"tree_method\": \"gpu_hist\" ,\"num_class\": 5}\n",
        "#XGB model generally used for further classification\n",
        "model2 = xgb.train(\n",
        "   params=params,\n",
        "   dtrain=dtrain_reg,\n",
        "   num_boost_round=10,\n",
        ")\n",
        "#XGB model used ultimately\n",
        "model3=xgb.XGBClassifier()\n",
        "model3.fit(X_train,Y_train)\n",
        "y_prepxg=model3.predict(X_test)\n",
        "ab=Y_test\n",
        "accuracyxg=accuracy_score(ab,y_prepxg)\n",
        "print(accuracyxg)\n",
        "#Confusion Matrix\n",
        "def cm(Y_train,y_test, y_pred):\n",
        "  conmat = confusion_matrix(y_test, y_pred)\n",
        "  val = np.mat(conmat)\n",
        "  classnames = list(Y_train)\n",
        "  df_cm = pd.DataFrame(val)\n",
        "  print(df_cm)\n",
        "cm(Y_train,Y_test,y_preplr)\n",
        "cm(Y_train,Y_test,y_preprf)\n",
        "cm(Y_train,Y_test,y_prepsv)\n",
        "cm(Y_train,Y_test,y_prepxg)\n",
        "#Use of Confusion Matrix and accuracy score signifies that use of Random Forest\n",
        "#is most beneficial"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all , an appropiate database containing variable names and non-biased data was found and used.\n",
        "Normalisation and imputation is implemented.Imputation method seemed better than using the other methods to remove the empty values(though there wasn't any such missing value in the database).The used of feature engineering isn't beneficial as the data used can't be further broke down, insetead it might degrade our model, thus its not implemented.Encoding to categorial variables was also not required as it was already implemented.Then SMOTE was implemented as it seemed to be the best under the situation of our model to take care of class imbalance.Then Logical Regression, Random Forests, XGBoost and SVM algorithms are implemented.Each model was evaluated using the accuracy score and confusion matrices. The comparison showed that the Random Forest method suits the best to our model.As the class imbalance was already implemented, use of precision-recall score, according to me, is reduntant and observable in the confusion Matrices. Also, Lightgbm was not implemented as it would be quite time consuming, and that our data isn't skewed enough to make it necessary to use it, as we didn't implement further feature and model evaluations.\n",
        "Results-\n",
        "Accuracy scores-\n",
        "RF-0.867\n",
        "LR-0.62\n",
        "SVM- 0.60\n",
        "XGBoost-0.834\n",
        "Confusion Matrices-\n",
        "LR-\n",
        "    0     1\n",
        "0  2834  1856\n",
        "1  1694  2962\n",
        "RF-\n",
        "     0     1\n",
        "0  4116   574\n",
        "1   667  3989\n",
        "SVM-\n",
        "      0     1\n",
        "0  3011  1679\n",
        "1  2014  2642\n",
        "XGB-\n",
        "      0     1\n",
        "0  4080   610\n",
        "1   941  3715\n",
        "**Thus Random Forest method seems the most appropiate"
      ],
      "metadata": {
        "id": "x_AmQd6Vc5tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SdAl4Pl1x5Sw"
      }
    }
  ]
}
